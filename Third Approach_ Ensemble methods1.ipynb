{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGsi7hA1ZMe5"
   },
   "source": [
    "In this approach, we try to combine our two first approaches with past kernels made available publicly on https://zindi.africa/hackathons/south-african-covid-19-vulnerability-map/discussions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2_1K5Rjv9dv"
   },
   "source": [
    "# Xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x3Qms9OkzG97"
   },
   "source": [
    "### Libraries versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRco5M0FzG-I"
   },
   "source": [
    "Please use these exact versions when rerunning the code for score reproducibility. A different version might yield a slightly different score.\n",
    "\n",
    "pandas : 1.0.3\n",
    "seaborn : 0.10.0\n",
    "matplotlib : 3.2.1\n",
    "numpy : 1.18.2\n",
    "scikit-learn : 0.22.2.post1\n",
    "lightgbm : 2.2.3\n",
    "xgboost : 0.7.post3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3veXQ97P_-I9"
   },
   "outputs": [],
   "source": [
    "## Reading libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kq9l5c804xy0"
   },
   "outputs": [],
   "source": [
    "## defining the metric\n",
    "def metric(x,y):\n",
    "  return np.sqrt(mean_squared_error(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Th04TmHUzpGy"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/content/drive/MyDrive/Carte de vulnérabilité du COVID-19 en Afrique du Sud by Nimba Hub 3,000,000 GNF/\") #Change the path to the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xj3P_tBGY-ps"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M_Aw9pjAQ_r"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('Data/Train_maskedv2.csv')\n",
    "test = pd.read_csv('Data/Test_maskedv2.csv')\n",
    "vdefinition = pd.read_csv('Data/variable_descriptions_v2.csv')\n",
    "submission = pd.read_csv('Data/samplesubmissionv2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7prRCBcUzG-L"
   },
   "source": [
    "###### KMeans and some data cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMOzd-lZxXlR"
   },
   "outputs": [],
   "source": [
    "### Applying Kmeans to almost all features and generating a 'cluster' feature.\n",
    "to_drop=['dw_11', 'dw_12','lan_13']\n",
    "train_copy=train.copy()\n",
    "columns=train_copy.drop([\"ward\",\"target_pct_vunerable\"]+to_drop,1).columns\n",
    "train_copy=train_copy[columns]\n",
    "km=KMeans(7,random_state=42)\n",
    "km=km.fit(train_copy[columns])\n",
    "train[\"cluster\"]=km.predict(train[columns])\n",
    "test[\"cluster\"]=km.predict(test[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04lyJA55KUjg"
   },
   "outputs": [],
   "source": [
    "## Dropping wards in the training data that have more than 17500 households + 1 outlier.\n",
    "train = train[train['total_households']<=17500]\n",
    "train = train[train.index!=1094]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3iglZOfSj4t"
   },
   "outputs": [],
   "source": [
    "## Binned feature on total_households\n",
    "train['total_householdslessthan5000'] = train['total_households'].apply(lambda x:1 if 2500<x<=5000  else 0)\n",
    "test['total_householdslessthan5000'] = test['total_households'].apply(lambda x:1 if 2500<x<=5000  else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbXeSzoxzG-N"
   },
   "source": [
    "###### A bunch of feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-JMmgLQBh5x"
   },
   "outputs": [],
   "source": [
    "train['Individualsperhouse'] = train['total_individuals'] / train['total_households']\n",
    "test['Individualsperhouse'] = test['total_individuals'] / test['total_households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH_4n3HY5SdY"
   },
   "outputs": [],
   "source": [
    "train['Luxury_01'] = train['car_01']+train['stv_00']+train['psa_01']\n",
    "train['Luxury_00'] = train['car_00'] +train['stv_01']+train['psa_00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiWuYbiwswbr"
   },
   "outputs": [],
   "source": [
    "test['Luxury_01'] = test['car_01']+test['stv_00']+test['psa_01']\n",
    "test['Luxury_00'] = test['car_00'] +test['stv_01']+test['psa_00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0SgfjfGmVnc"
   },
   "outputs": [],
   "source": [
    "train['NoSchoolAttendace'] = train['psa_01'] + train['psa_02']+ train['psa_03']\n",
    "test['NoSchoolAttendace'] = test['psa_01'] + test['psa_02']+ test['psa_03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2MoFf5Rri6U"
   },
   "outputs": [],
   "source": [
    "train['InformalDwellings'] = train['dw_02'] + train['dw_07'] + train['dw_06']\n",
    "test['InformalDwellings'] = test['dw_02'] + test['dw_07'] + test['dw_06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf6z67clzofh"
   },
   "outputs": [],
   "source": [
    "train['TraditionalVSInformalDwellings'] = np.absolute(train['dw_01'] - train['dw_08'])\n",
    "test['TraditionalVSInformalDwellings'] = np.absolute(test['dw_01'] - test['dw_08'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh_5AH3BSBiP"
   },
   "outputs": [],
   "source": [
    "train['total_households']/=train['total_households'].max()\n",
    "train['total_individuals']/=train['total_individuals'].max()\n",
    "\n",
    "test['total_households']/=test['total_households'].max()\n",
    "test['total_individuals']/=test['total_individuals'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h-6ztZ3lT88"
   },
   "outputs": [],
   "source": [
    "train['SAOldPeopleSesothoVSSetswana'] = np.absolute(train['lan_06'] - train['lan_07'])\n",
    "\n",
    "test['SAOldPeopleSesothoVSSetswana'] = np.absolute(test['lan_06'] - test['lan_07'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2y6ZHN6zG-R"
   },
   "source": [
    "###### Target encoding + PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecbR6lup6CgU"
   },
   "outputs": [],
   "source": [
    "target_mean = train.groupby(['cluster']).mean()[['target_pct_vunerable']]\n",
    "for i in list(target_mean.columns):\n",
    "  target_mean.rename({i:i+\"_mean\"},axis=1,inplace=True)\n",
    "train = train.merge(target_mean,how=\"left\",on='cluster')\n",
    "test = test.merge(target_mean,how=\"left\",on='cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eLDTE4h5rId"
   },
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42,n_components=1)\n",
    "pg_features =  train.filter(regex='lan_.*')\n",
    "train_pca = pca.fit_transform(pg_features)\n",
    "train['pca_lan_0'] = train_pca[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-qO-JbF6HEV"
   },
   "outputs": [],
   "source": [
    "pg_features =  test.filter(regex='lan_.*')\n",
    "test_pca = pca.transform(pg_features)\n",
    "test['pca_lan_0'] = test_pca[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BViFYC6EzG-T"
   },
   "source": [
    "###### Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jptOCWh0Wocv"
   },
   "outputs": [],
   "source": [
    "target = train['target_pct_vunerable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_aJZuhzm_wx"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['psa_00','psa_02','psa_03','psa_04','psa_01','lgt_00','stv_01','car_01','lln_01','ward','dw_12','dw_13','lan_13','target_pct_vunerable'], axis=1)\n",
    "test = test.drop(['psa_00','psa_02','psa_03','psa_04','psa_01','lgt_00','stv_01','car_01','lln_01','ward','dw_12','dw_13','lan_13'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzE4LBq2zG-U"
   },
   "source": [
    "###### a 4 folds averaging solution using xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VwsGZzY1WkT-"
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=4,shuffle=False)\n",
    "lgbm = XGBRegressor(n_estimators=50000,random_state=42,max_depth=5,learning_rate=0.03888)\n",
    "scores = []\n",
    "pred_test = np.zeros(len(test))\n",
    "for (train_index,test_index) in kf.split(train,target):\n",
    "  X_train,X_test = train.iloc[train_index],train.iloc[test_index]\n",
    "  y_train,y_test = target.iloc[train_index],target.iloc[test_index]\n",
    "  lgbm.fit(X_train,y_train,early_stopping_rounds=500,eval_set=[(X_test,y_test)],eval_metric='rmse')\n",
    "  scores.append(metric(lgbm.predict(X_test),y_test))\n",
    "  pred_test+=lgbm.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96SZF-SKJ11l"
   },
   "outputs": [],
   "source": [
    "np.mean(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tblgsxyNzG-X"
   },
   "source": [
    "###### Plotting features importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuSQwaeR0HeR"
   },
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# sorted(zip(clf.feature_importances_, X.columns), reverse=True)\n",
    "feature_imp = pd.DataFrame(sorted(zip(lgbm.feature_importances_,train.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(30, 20))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('XGB Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jlLjR-mzG-Y"
   },
   "source": [
    "###### And finally the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOaQE76k-368"
   },
   "outputs": [],
   "source": [
    "submission['target_pct_vunerable'] = np.absolute(pred_test/4)\n",
    "submission.to_csv('Submissions/first.csv',index=False)\n",
    "submission['target_pct_vunerable'] = np.clip(submission['target_pct_vunerable'], a_min=0, a_max=90)\n",
    "submission.to_csv('Submissions/first_cliped.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YlZkeXcDwLat"
   },
   "source": [
    "# Models Combinaison approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_lKOqzNxEqi"
   },
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iecWxY_4xGt5"
   },
   "source": [
    "\n",
    "Can we infer important COVID-19 public health risk factors from outdated data? In many countries census and other survey data may be incomplete or out of date. This challenge is to develop a proof-of-concept for how machine learning can help governments more accurately map COVID-19 risk in 2020 using old data, without requiring a new costly, risky, and time-consuming on-the-ground survey.\n",
    "\n",
    "The 2011 census gives us valuable information for determining who might be most vulnerable to COVID-19 in South Africa. However, the data is nearly 10 years old, and we expect that some key indicators will have changed in that time. Building an up-to-date map showing where the most vulnerable are located will be a key step in responding to the disease. A mapping effort like this requires bringing together many different inputs and tools. For this competition, we’re starting small. Can we infer important risk factors from more readily available data?\n",
    "\n",
    "The task is to predict the percentage of households that fall into a particularly vulnerable bracket - large households who must leave their homes to fetch water - using 2011 South African census data. Solving this challenge will show that with machine learning it is possible to use easy-to-measure stats to identify areas most at risk even in years when census data is not collected.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFlK13sixKtl"
   },
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jEs9vN1MxN3_"
   },
   "outputs": [],
   "source": [
    "# Installing the necessary libraries\n",
    "!pip install catboost\n",
    "!pip install rgf-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1lV61NB4xPLg"
   },
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n",
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q xgboost\n",
    "!pip install -q lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4zCGJtsWxRq2"
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from io import StringIO \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVR, NuSVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from xgboost import XGBRegressor, XGBRFRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, BayesianRidge\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor,HistGradientBoostingRegressor, ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "import warnings\n",
    "from rgf.sklearn import RGFRegressor\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGwO_CRMxYPd"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ht4SQuDZxdwX"
   },
   "outputs": [],
   "source": [
    "# Created links to shared files via google drive\n",
    "#\n",
    "train = 'https://drive.google.com/file/d/1_wLi9i-pUk6Kaizjb5i6-Pd0Vi7L1d-E/view?usp=sharing'\n",
    "test = 'https://drive.google.com/file/d/1OeT53v7tZLnB71n1j4r6KFbtrouz38ej/view?usp=sharing'\n",
    "\n",
    "# Created a function to read a csv file shared via google and return a dataframe\n",
    "#\n",
    "def read_csv(url):\n",
    "    url = 'https://drive.google.com/uc?export=download&id=' + url.split('/')[-2]\n",
    "    csv_raw = requests.get(url).text\n",
    "    csv = StringIO(csv_raw)\n",
    "    df = pd.read_csv(csv)\n",
    "    return df\n",
    "\n",
    "# Creating training and testing datataframes\n",
    "#\n",
    "train = read_csv(train)\n",
    "test = read_csv(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv-qbwwPxiwt"
   },
   "source": [
    "## Combining the training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "zBFqXzPKxn_e"
   },
   "outputs": [],
   "source": [
    "# Combining test and train for easy feature engineering.\n",
    "target = train.target_pct_vunerable\n",
    "\n",
    "train['separator'] = 0\n",
    "test['separator'] = 1\n",
    "\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "\n",
    "comb = pd.concat([train, test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTKi4_uhxo3u"
   },
   "source": [
    "\n",
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "RR5S0eAcxuBU"
   },
   "outputs": [],
   "source": [
    "# Examining feature interactions from the most important features from model's feature importances graph and creating new magic features.\n",
    "# While there is no science into it and it's mostly trial and error, the new features improved the score greatly and if we had computational power, \n",
    "# we could have explored more interactions.\n",
    "\n",
    "comb['household_size'] = comb.total_individuals/comb.total_households\n",
    "comb['gf_1'] = comb['dw_01'] * comb['psa_01']\n",
    "comb['gf_2'] = comb['gf_1'] * comb['psa_00']\n",
    "comb['gf_3'] = comb['gf_1'] * comb['psa_02']\n",
    "comb['gf_4'] = comb['gf_1'] * comb['psa_03']\n",
    "comb['gf_5'] = comb['gf_1'] * comb['gf_2']\n",
    "comb['gf_6'] = comb['gf_5'] * comb['gf_2']\n",
    "comb['dw_01_2'] = comb['dw_01'] ** 2\n",
    "comb['psa_00_2'] = comb['psa_00'] ** 2\n",
    "luxury_stuff = ['psa_01','car_01','stv_00']\n",
    "not_luxury_stuff = ['psa_00','car_00','stv_01']\n",
    "comb['luxury_stuff'] = comb[luxury_stuff].sum(axis=1)\n",
    "comb['not_luxury_stuff'] = comb[not_luxury_stuff].sum(axis=1)\n",
    "comb['a_luxury_stuff'] = comb[luxury_stuff].mean(axis=1)\n",
    "comb['a_not_luxury_stuff'] = comb[not_luxury_stuff].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7QzNBcDxwaB"
   },
   "source": [
    "## Separating train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8As5i__2xxP8"
   },
   "outputs": [],
   "source": [
    "# Separating the train and test datasets.\n",
    "train = comb[comb.separator == 0]\n",
    "test = comb[comb.separator == 1]\n",
    "\n",
    "train.drop('separator', axis = 1, inplace = True)\n",
    "test.drop('separator', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhac4hToxz5h"
   },
   "source": [
    "## Splitting training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Pl-sE5m5x0oQ"
   },
   "outputs": [],
   "source": [
    "# The columns dropped were those that from the feature importance of the baseline model, were of least importance and just added noise to the model.\n",
    "\n",
    "X = train.drop(columns=['ward', 'dw_13', 'dw_12', 'lan_13', 'psa_03'])\n",
    "y = target.copy()\n",
    "tes = test.drop(['ward', 'dw_13', 'dw_12', 'lan_13', 'psa_03'], 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N4Bha_0bx7cV"
   },
   "source": [
    "## Training different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "IM-rvb9Fx6r1"
   },
   "outputs": [],
   "source": [
    "\n",
    "# In stacking, the most important thing is model diversification. from linear, SVM, KNN and Decision trees and many variations of them. \n",
    "# The variations are different values of key parameters of each model. \n",
    "# While we did not have the time to tune parameters of each model, except the meta learner Catboost, educated guesses on \n",
    "# the parameters were made to have as much variability as possible.\n",
    "\n",
    "estimators_1 = [\n",
    "    ('xgb', XGBRegressor(random_state=2020, objective ='reg:squarederror', learning_rate=0.05)),\n",
    "    ('lr', LinearRegression()),\n",
    "    ('rf', RandomForestRegressor(random_state=2020)),\n",
    "    ('lgb', LGBMRegressor(learning_rate=0.2, random_state=2020)),\n",
    "    ('svr', SVR(degree=2)),\n",
    "    ('lasso', Lasso(random_state=2020)),\n",
    "    ('RGF', RGFRegressor()),\n",
    "    ('kneiba', KNeighborsRegressor(n_neighbors=4)),\n",
    "    ('cat', CatBoostRegressor(logging_level='Silent', random_state=2020))\n",
    "]\n",
    "\n",
    "predictions_1 = StackingRegressor(estimators=estimators_1, final_estimator=CatBoostRegressor(logging_level='Silent', depth=6, bagging_temperature=5, random_state=2020)).fit(X_train, y_train).predict(tes)\n",
    "\n",
    "estimators_2 = [\n",
    "    ('xgb', XGBRegressor(objective ='reg:squarederror', learning_rate=0.2, random_state=2020)),\n",
    "    ('lr', LinearRegression()),\n",
    "    ('rf', RandomForestRegressor(random_state=2020)),\n",
    "    ('lgb', LGBMRegressor(learning_rate=0.05, random_state=2020)),\n",
    "    ('svr', SVR(degree=5)),\n",
    "    ('RGF', RGFRegressor()),\n",
    "    ('lasso', Lasso(random_state=2020)),\n",
    "    ('kneiba', KNeighborsRegressor(n_neighbors=6)),\n",
    "    ('cat', CatBoostRegressor(logging_level='Silent', random_state=2020))\n",
    "]\n",
    "\n",
    "predictions_2 = StackingRegressor(estimators=estimators_2, final_estimator=CatBoostRegressor(logging_level='Silent', depth=6, bagging_temperature=5, random_state=2020)).fit(X_train, y_train).predict(tes)\n",
    "\n",
    "predictions_cat_1 = CatBoostRegressor(logging_level='Silent', depth=6, bagging_temperature=5, random_state=2020).fit(X_train, y_train).predict(tes)\n",
    "\n",
    "\n",
    "# Further averaging, blending and retraining to generalise well\n",
    "# While the ratios are greater than one, it still works a treat. This is definitely one of the parameters to tune to achieve great results.\n",
    "stack = [x*0.56 + y*0.51 for x, y in zip(predictions_1, predictions_2)]\n",
    "stack_2 = [x*0.56 + y*0.51 for x, y in zip(stack, predictions_cat_1)]\n",
    "\n",
    "X,y = tes.copy(), stack_2\n",
    "preds_ridge = Ridge(random_state=2020).fit(X, y).predict(X)\n",
    "\n",
    "# We added a new feature to the test dataset, where we clustered the wards to 150 clusters, then used Catboost's encoder to encode the clusters.\n",
    "X['cluster'] = KMeans(150, random_state=2020).fit(X).predict(X)\n",
    "preds_cat = CatBoostRegressor(random_state=2020, verbose = False, depth=6, bagging_temperature=5, cat_features=['cluster']).fit(X, y).predict(X)\n",
    "\n",
    "# blended the Ridge and Catboost predictions.\n",
    "final_blend_2 = [x*0.2 +y*0.8 for x, y in zip(preds_ridge, preds_cat)]\n",
    "\n",
    "# Clipping the values from between 0 - 90 was also important as we know that the target variable is between 0 to 100.\n",
    "final_blend_2 = np.clip(final_blend_2, a_min=0, a_max=90)\n",
    "\n",
    "# Applying regularization to the final blend by substracting a constant from the predictions and clipping again.\n",
    "exp = final_blend_2 - 0.48\n",
    "exp = np.clip(exp, a_min=0, a_max=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0PBR_HgJyKQd"
   },
   "source": [
    "## Retraining predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "6Vsc8kNMyNe1"
   },
   "outputs": [],
   "source": [
    "# Retraining on the test data by using the prediction of the stacked regressors as our target.\n",
    "# We also added the clusters but had to manually mean encode the clusters to the target variable as LinearRegression cannot encode categorical variables.\n",
    "X = tes.copy()\n",
    "\n",
    "X['cluster'] = KMeans(150, random_state=2020).fit(X).predict(X)\n",
    "X['target'] = exp\n",
    "X['encoded'] = X['cluster'].map(X.groupby('cluster')['target'].mean())\n",
    "y=X.target\n",
    "X=X.drop(['cluster', 'target'], 1)\n",
    "preds_1 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.7 + LinearRegression().fit(X, y).predict(X)*0.3\n",
    "preds_2 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.5 + LinearRegression().fit(X, y).predict(X)*0.5\n",
    "preds_3 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.6 + LinearRegression().fit(X, y).predict(X)*0.4\n",
    "\n",
    "final = [x*0.3 + y*0.3 + z*0.4 for x, y, z in zip(preds_1, preds_2, preds_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do5wjzBayTk_"
   },
   "source": [
    "## Further retraining of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "caRqBeIDwM55"
   },
   "outputs": [],
   "source": [
    "# Retraining again this time using Regularized Greedy Forests and Catboost.\n",
    "X['final'] = final\n",
    "y = X.final\n",
    "X = X.drop('final', 1)\n",
    "preds_1 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.7 + RGFRegressor().fit(X, y).predict(X)*0.3\n",
    "preds_2 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.5 + RGFRegressor().fit(X, y).predict(X)*0.5\n",
    "preds_3 = CatBoostRegressor(verbose = False, random_state=2020).fit(X,y).predict(X)*0.6 + RGFRegressor().fit(X, y).predict(X)*0.4\n",
    "\n",
    "final2 = [x*0.3 + y*0.3 + z*0.4 for x, y, z in zip(preds_1, preds_2, preds_3)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1LnbBKMyYe6"
   },
   "source": [
    "## Creating a submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "FTcxr43dyY7J"
   },
   "outputs": [],
   "source": [
    "# Clipping for the final time and creating the submission file.\n",
    "final2 = np.clip(final2, a_min=0, a_max=90)\n",
    "sub_df = pd.DataFrame({'ward': test.ward, 'target_pct_vunerable': final2-0.2})\n",
    "sub_df.to_csv('../Submissions/second.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOtgNdxGycW_"
   },
   "source": [
    "## Challenges faced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Y11m7DoyeUl"
   },
   "source": [
    "We faced a problem of reproducibility as the score was changing with each submission with no change in code. However, that was solved by setting the *random_state* parameter of all models that have it to the same value. Now, the solution provides a consistently similar score each time it's rerun.\n",
    "\n",
    "\n",
    "However, the solution has a better private Leader board score of *3.50354986128398* which is better than the score we uploaded in time which was *3.52760301028188*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAJoCp0R1W4c"
   },
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/root/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f4ZkVutL3jqu"
   },
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "pd.set_option('max_column', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Bq8bLwf03jq6"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Data/Train_maskedv2.csv')\n",
    "test = pd.read_csv('../Data/Test_maskedv2.csv')\n",
    "vdefinition = pd.read_csv('../Data/variable_descriptions_v2.csv')\n",
    "submission = pd.read_csv('../Data/samplesubmissionv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "P7-Y3SSm3jq7"
   },
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth', 200)\n",
    "pd.set_option('max_info_rows', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "VEvjUYum3jq7"
   },
   "outputs": [],
   "source": [
    "train.drop(columns=['dw_12', 'dw_13', 'lan_13'], inplace=True)\n",
    "test.drop(columns=['dw_12', 'dw_13', 'lan_13'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "62B0IkkT3jq8"
   },
   "outputs": [],
   "source": [
    "train_len = len(train)\n",
    "data=pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "Kalyqizg3jq8"
   },
   "outputs": [],
   "source": [
    "data['rich'] = data['car_01']+data['stv_01']+data['psa_01']+data['dw_02']+data['lln_00']\n",
    "data['poor'] = data['car_00'] +data['stv_00']+data['psa_00']+data['dw_01']+data['lln_01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "lgfdIL513jq9"
   },
   "outputs": [],
   "source": [
    "data['household_size'] =data['total_individuals'] / data['total_households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "er0UxDOK3jq9"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "columns=data.drop([\"target_pct_vunerable\",\"ward\"],1).columns\n",
    "\n",
    "data_km=data[columns].copy()\n",
    "\n",
    "data_km[\"total_households\"]/=data_km[\"total_households\"].max()\n",
    "data_km[\"total_individuals\"]/=data_km[\"total_individuals\"].max()\n",
    "\n",
    "km=KMeans(15,random_state=2019)\n",
    "data[\"cluster\"]=km.fit_predict(data_km[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "nmimONt23jq-"
   },
   "outputs": [],
   "source": [
    "train = data[:train_len]\n",
    "test = data[train_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "qJGwNa5r3jq-"
   },
   "outputs": [],
   "source": [
    "_id = test['ward']\n",
    "test.drop(columns=['target_pct_vunerable','ward'], inplace=True)\n",
    "train.drop(columns=['ward'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "oZvjOn0a3jq_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3174, 50), (1102, 49))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "trpbcoeI3jq_"
   },
   "outputs": [],
   "source": [
    "train['total_households'] = np.log10(train['total_households'])\n",
    "test['total_households'] = np.log10(test['total_households'])\n",
    "\n",
    "train['total_individuals'] = np.log10(train['total_individuals'])\n",
    "test['total_individuals'] = np.log10(test['total_individuals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "OWFiDuDZ3jrA"
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=['target_pct_vunerable'])\n",
    "y = train['target_pct_vunerable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "CWNvLP4A3jrA",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['total_households', 'total_individuals', 'dw_00', 'dw_01', 'dw_02',\n",
       "       'dw_03', 'dw_04', 'dw_05', 'dw_06', 'dw_07', 'dw_08', 'dw_09', 'dw_10',\n",
       "       'dw_11', 'psa_00', 'psa_01', 'psa_02', 'psa_03', 'psa_04', 'stv_00',\n",
       "       'stv_01', 'car_00', 'car_01', 'lln_00', 'lln_01', 'lan_00', 'lan_01',\n",
       "       'lan_02', 'lan_03', 'lan_04', 'lan_05', 'lan_06', 'lan_07', 'lan_08',\n",
       "       'lan_09', 'lan_10', 'lan_11', 'lan_12', 'lan_14', 'pg_00', 'pg_01',\n",
       "       'pg_02', 'pg_03', 'pg_04', 'lgt_00', 'rich', 'poor', 'household_size',\n",
       "       'cluster'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "hb6Ewnh33jrA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.isna().sum().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rgf.sklearn import RGFRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "UaefdprG3jrB"
   },
   "outputs": [],
   "source": [
    "col = ['car_00', 'car_01', 'dw_00', 'dw_01', 'dw_02', 'dw_03', 'dw_04',\n",
    "       'lan_08', 'lan_09', 'lan_10', 'lan_11', 'lan_12', 'lan_14', 'lgt_00',\n",
    "       'dw_05', 'dw_06', 'dw_07', 'dw_08', 'dw_09', 'dw_10', 'dw_11', 'lan_00',\n",
    "       'lan_01', 'lan_02', 'lan_03', 'lan_04', 'lan_05', 'lan_06', 'lan_07',\n",
    "       'lln_00', 'lln_01', 'pg_00', 'pg_01', 'pg_02', 'pg_03', 'pg_04',\n",
    "       'psa_00', 'psa_01', 'psa_02', 'psa_03', 'psa_04', 'stv_00', 'stv_01',\n",
    "        'rich', 'poor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "lhKL724L3jrB"
   },
   "outputs": [],
   "source": [
    "X[col] = X[col].round(2)\n",
    "test[col] = test[col].round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "b0somMqL3jrB"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([48])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features_indices = np.where(X.dtypes != np.float)[0]; categorical_features_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbUne8aC3jrC"
   },
   "outputs": [],
   "source": [
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold,StratifiedKFold, TimeSeriesSplit\n",
    "\n",
    "testsplit_store=[]\n",
    "test_store=[]\n",
    "fold=KFold(n_splits=15, shuffle=True, random_state=123456)\n",
    "i=1\n",
    "for train_index, test_index in fold.split(X,y):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    cat = CatBoostRegressor(n_estimators=10000,eval_metric='RMSE', learning_rate=0.0801032, random_seed= 123456, l2_leaf_reg=4, use_best_model=True)\n",
    "    cat.fit(X_train,y_train,eval_set=[(X_train,y_train),(X_test, y_test)], early_stopping_rounds=300,verbose=100, cat_features=categorical_features_indices)\n",
    "    predict = cat.predict(X_test)\n",
    "    print(\"err: \",np.sqrt(mean_squared_error(y_test,predict)))\n",
    "    testsplit_store.append(np.sqrt(mean_squared_error(y_test,predict)))\n",
    "    pred = cat.predict(test)\n",
    "    test_store.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMEpioHb3jrD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "np.mean(testsplit_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azM9ym3t3jrD"
   },
   "outputs": [],
   "source": [
    "submit_prep = {\"ward\": _id, 'target_pct_vunerable': np.mean(test_store, 0)}\n",
    "submission = pd.DataFrame(data = submit_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YgFI2dMB3jrD"
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwZYb4CB3jrE"
   },
   "outputs": [],
   "source": [
    "submission.to_csv('Submissions/third.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejQLQmhP4TKj"
   },
   "outputs": [],
   "source": [
    "submission['target_pct_vunerable'] = np.clip(submission['target_pct_vunerable'], a_min=0, a_max=90)\n",
    "submission.to_csv('Submissions/third_cliped.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2wwTVQo93jrE"
   },
   "outputs": [],
   "source": [
    "## Check for the feature importance \n",
    "fea_imp = pd.DataFrame({'imp':cat.feature_importances_, 'col': X.columns})\n",
    "fea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-60:]\n",
    "_ = fea_imp.plot(kind='barh', x='col', y='imp', figsize=(20, 10))\n",
    "plt.savefig('catboost_feature_importance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2bnvJNb4dI0"
   },
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTr6kpFA4ZKn"
   },
   "outputs": [],
   "source": [
    "s1 = pd.read_csv(\"Submissions/first.csv\")\n",
    "s2 = pd.read_csv(\"Submissions/second.csv\")\n",
    "s3 = pd.read_csv(\"Submissions/alpha_beta.csv\")\n",
    "s4 = pd.read_csv(\"Submissions/third.csv\")\n",
    "submission['target_pct_vunerable']=(s1['target_pct_vunerable']+s2['target_pct_vunerable']+s3['target_pct_vunerable ']+s4['target_pct_vunerable'])/4\n",
    "submission.to_csv('Submissions/'+'assembling_uniform.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ydWxIjCGXhg"
   },
   "outputs": [],
   "source": [
    "#Best submission\n",
    "a_min, a_max = 0.099, 47\n",
    "submission['target_pct_vunerable']=np.clip((s1['target_pct_vunerable']+s2['target_pct_vunerable'])/2, a_min=a_min, a_max=a_max)\n",
    "submission.to_csv('Submissions/'+f'assembling_uniform_first_second_clipped_{a_min}_{a_max}.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CL8bblFHSCvq"
   },
   "source": [
    "### Others trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcgrLM8XTcqw"
   },
   "outputs": [],
   "source": [
    "alpha = pd.read_csv(\"Submissions/alpha_voting.csv\")\n",
    "beta = pd.read_csv(\"Submissions/beta_voting.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uzTf0U4TmGI"
   },
   "outputs": [],
   "source": [
    "a_min, a_max = 0.099, 47\n",
    "submission['target_pct_vunerable']=np.clip(alpha['target_pct_vunerable '], a_min=a_min, a_max=a_max)\n",
    "submission.to_csv('Submissions/'+f'alpha_voting_clipped_{a_min}_{a_max}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TH2fti0DT3EJ"
   },
   "outputs": [],
   "source": [
    "a_min, a_max = 0.099, 47\n",
    "submission['target_pct_vunerable']=np.clip(beta['target_pct_vunerable '], a_min=a_min, a_max=a_max)\n",
    "submission.to_csv('Submissions/'+f'beta_voting_clipped_{a_min}_{a_max}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KTkfRPm-5A--"
   },
   "outputs": [],
   "source": [
    "submission['target_pct_vunerable']=0.3*s1['target_pct_vunerable']+0.5*s2['target_pct_vunerable']+0.2*s3['target_pct_vunerable ']+0.2*s4['target_pct_vunerable']\n",
    "submission.to_csv('Submissions/'+'assembling_weighted.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SUVWKt_p5QBD"
   },
   "outputs": [],
   "source": [
    "s4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBgIbtqD5SJJ"
   },
   "outputs": [],
   "source": [
    "a_min, a_max = 0.099, 47\n",
    "submission['target_pct_vunerable']=np.clip((s1['target_pct_vunerable']+s2['target_pct_vunerable']+s3['target_pct_vunerable ']+s4['target_pct_vunerable'])/4, a_min=a_min, a_max=a_max)\n",
    "submission.to_csv('Submissions/'+f'assembling_uniform_clipped_{a_min}_{a_max}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqpAAJy3GzPV"
   },
   "outputs": [],
   "source": [
    "w=0.4\n",
    "submission['target_pct_vunerable']=np.clip(w1*s1['target_pct_vunerable']+(1-w)*s2['target_pct_vunerable'], a_min=0.099, a_max=47)\n",
    "submission.to_csv('Submissions/'+f'assembling_first_second_weighted_{w}_clipped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cL4jBfxT6pgg"
   },
   "outputs": [],
   "source": [
    "dif_1 = pd.read_csv(\"Submissions/tabnet_best_params_cliped_0.06_47.csv\")\n",
    "dif_2 = pd.read_csv(\"Submissions/assembling_uniform_clipped_0.06_47.csv\")\n",
    "submission['target_pct_vunerable']=(dif_1['target_pct_vunerable']+dif_2['target_pct_vunerable'])/2\n",
    "submission.to_csv('Submissions/'+'assembling_uniform_tabnet_f_ass.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EtmikcVo9lc-"
   },
   "outputs": [],
   "source": [
    "w1=0.035\n",
    "a_min, a_max = 0.06, 47\n",
    "submission['target_pct_vunerable']=np.clip(w1*dif_1['target_pct_vunerable']+(1-w1)*dif_2['target_pct_vunerable'], a_min=0.06, a_max =47)\n",
    "submission.to_csv('Submissions/'+f'assembling_uniform_tabnet_f_ass_weighted_{w1}_clipped_{a_min}_{a_max}.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wlO07PQI92_6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO6B0IoP1vZsjZdZoVsKZx+",
   "collapsed_sections": [
    "1lV61NB4xPLg",
    "Wv-qbwwPxiwt",
    "PTKi4_uhxo3u",
    "-7QzNBcDxwaB",
    "xhac4hToxz5h",
    "0PBR_HgJyKQd"
   ],
   "mount_file_id": "1Mhi13Ra8e-QVRKW-VXxwi_FFGbTdgHGG",
   "name": "Third Approach: Ensemble methods.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
