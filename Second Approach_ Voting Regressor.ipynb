{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRM9yRN-zott"
   },
   "outputs": [],
   "source": [
    "! pip install -q catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ffGETOUM9gaq"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/Carte de vulnérabilité du COVID-19 en Afrique du Sud by Nimba Hub 3,000,000 GNF/' # change this path to the location where you put your data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLZfCF30itp"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49iAGgvezrjC"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures, PowerTransformer, KBinsDiscretizer, Normalizer, QuantileTransformer, FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV,cross_val_score,KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.svm import SVR, LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,VotingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", path]).decode(\"utf8\")) #check the files available in the directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpkzgWn90lJS"
   },
   "source": [
    "# Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxyPQpMqcfaz"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(path+'Data/Test_maskedv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLKZM5AsclUR"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(path+'Data/Train_maskedv2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "furmRm_ocw7v"
   },
   "outputs": [],
   "source": [
    "varaiable_descr = pd.read_csv(path+ 'Data/variable_descriptions_v2.csv')\n",
    "varaiable_descr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2zEecRI9zLH"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4jrtKSD9T_8"
   },
   "outputs": [],
   "source": [
    "# Checking shape\n",
    "#check the numbers of samples and features\n",
    "print(\"The train data size before dropping ward feature is : {} \".format(train.shape))\n",
    "print(\"The test data size before dropping ward feature is : {} \".format(test.shape))\n",
    "\n",
    "#Save the 'Id' column\n",
    "train_ward = train['ward']\n",
    "test_ward = test['ward']\n",
    "\n",
    "#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(\"ward\", axis = 1, inplace = True)\n",
    "test.drop(\"ward\", axis = 1, inplace = True)\n",
    "\n",
    "#check again the data size after dropping the 'ward' variable\n",
    "print(\"\\nThe train data size after dropping ward feature is : {} \".format(train.shape)) \n",
    "print(\"The test data size after dropping ward feature is : {} \".format(test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "su0rPxMy0tlE"
   },
   "outputs": [],
   "source": [
    "#checking missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY-WA2bZ-XP2"
   },
   "outputs": [],
   "source": [
    "test.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9DCVk7Mg-cPo"
   },
   "outputs": [],
   "source": [
    "#Checking duplicates\n",
    "print(\"Number of duplicates in train:\", train.duplicated().sum())\n",
    "print(\"Number of duplicates in test:\", test.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fCz8k3Se-xd3"
   },
   "outputs": [],
   "source": [
    "#Checking the number of unique values per columns (specially if they don't vary at all)\n",
    "for col in train.columns:\n",
    "  if train[col].nunique() <=1:\n",
    "    print('variable name: ', col, '\\n',\n",
    "          'variable description :', varaiable_descr[varaiable_descr['Column']==col]['Description'].values[0], '\\n',\n",
    "          'number_of_unique_values: ', train[col].nunique(), '\\n', \n",
    "          'unique values: ', train[col].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oeZ0Ct3k_UlG"
   },
   "outputs": [],
   "source": [
    "#Checking the number of unique values per columns (specially if they don't vary at all)\n",
    "for col in test.columns:\n",
    "  if test[col].nunique() <=1:\n",
    "    print('variable name: ', col, '\\n',\n",
    "          'variable description :', varaiable_descr[varaiable_descr['Column']==col]['Description'].values[0], '\\n',\n",
    "          'number_of_unique_values: ', test[col].nunique(), '\\n', \n",
    "          'unique values: ', test[col].unique(), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8bw8X3ajBbN7"
   },
   "outputs": [],
   "source": [
    "# droping invariant features\n",
    "train= train.drop(['dw_12', 'dw_13', 'lan_13'], axis=1)\n",
    "test= test.drop(['dw_12', 'dw_13', 'lan_13'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGL1R40uBoOH"
   },
   "outputs": [],
   "source": [
    "def outlier_checker(col):\n",
    "  \"\"\"\n",
    "  function taking in argument a culumn name of the train\n",
    "  dataframe and return its scatterplot with the dependant variable\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots()\n",
    "  ax.scatter(x = train[col], y = train['target_pct_vunerable'])\n",
    "  plt.ylabel('Pourcentage vulnerable', fontsize=13)\n",
    "  plt.xlabel(f'{col}', fontsize=13)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lenk9DViB8qQ"
   },
   "outputs": [],
   "source": [
    "# Let's visualize these scatters\n",
    "for col in train.columns:\n",
    "  outlier_checker(col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "itYoU47Q-EZM"
   },
   "source": [
    "**We choose to keep our outliers**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D9RliNG4CoGE"
   },
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L5hqMZ1_Cgvb"
   },
   "outputs": [],
   "source": [
    "train['avg_total_num_of_ind_per_households'] = (train['total_individuals']/train['total_households'])*100\n",
    "test['avg_total_num_of_ind_per_households'] = (test['total_individuals']/test['total_households'])*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sojk6eORAtKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMOzd-lZxXlR"
   },
   "outputs": [],
   "source": [
    "### Applying Kmeans to almost all features and generating a 'cluster' feature.\n",
    "train_copy=train.copy()\n",
    "columns=train_copy.drop([\"target_pct_vunerable\"],1).columns\n",
    "train_copy=train_copy[columns]\n",
    "km=KMeans(7,random_state=42)\n",
    "km=km.fit(train_copy[columns])\n",
    "train[\"cluster\"]=km.predict(train[columns])\n",
    "test[\"cluster\"]=km.predict(test[columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04lyJA55KUjg"
   },
   "outputs": [],
   "source": [
    "## Dropping wards in the training data that have more than 17500 households + 1 outlier.\n",
    "train = train[train['total_households']<=17500]\n",
    "train = train[train.index!=1094]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q3iglZOfSj4t"
   },
   "outputs": [],
   "source": [
    "## Binned feature on total_households\n",
    "train['total_householdslessthan5000'] = train['total_households'].apply(lambda x:1 if 2500<x<=5000  else 0)\n",
    "test['total_householdslessthan5000'] = test['total_households'].apply(lambda x:1 if 2500<x<=5000  else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQflR6G66OGH"
   },
   "source": [
    "###### A bunch of feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U-JMmgLQBh5x"
   },
   "outputs": [],
   "source": [
    "train['Individualsperhouse'] = train['total_individuals'] / train['total_households']\n",
    "test['Individualsperhouse'] = test['total_individuals'] / test['total_households']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tH_4n3HY5SdY"
   },
   "outputs": [],
   "source": [
    "train['Luxury_01'] = train['car_01']+train['stv_00']+train['psa_01']\n",
    "train['Luxury_00'] = train['car_00'] +train['stv_01']+train['psa_00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiWuYbiwswbr"
   },
   "outputs": [],
   "source": [
    "test['Luxury_01'] = test['car_01']+test['stv_00']+test['psa_01']\n",
    "test['Luxury_00'] = test['car_00'] +test['stv_01']+test['psa_00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0SgfjfGmVnc"
   },
   "outputs": [],
   "source": [
    "train['NoSchoolAttendace'] = train['psa_01'] + train['psa_02']+ train['psa_03']\n",
    "test['NoSchoolAttendace'] = test['psa_01'] + test['psa_02']+ test['psa_03']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2MoFf5Rri6U"
   },
   "outputs": [],
   "source": [
    "train['InformalDwellings'] = train['dw_02'] + train['dw_07'] + train['dw_06']\n",
    "test['InformalDwellings'] = test['dw_02'] + test['dw_07'] + test['dw_06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf6z67clzofh"
   },
   "outputs": [],
   "source": [
    "train['TraditionalVSInformalDwellings'] = np.absolute(train['dw_01'] - train['dw_08'])\n",
    "test['TraditionalVSInformalDwellings'] = np.absolute(test['dw_01'] - test['dw_08'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eh_5AH3BSBiP"
   },
   "outputs": [],
   "source": [
    "train['total_households']/=train['total_households'].max()\n",
    "train['total_individuals']/=train['total_individuals'].max()\n",
    "\n",
    "test['total_households']/=test['total_households'].max()\n",
    "test['total_individuals']/=test['total_individuals'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9h-6ztZ3lT88"
   },
   "outputs": [],
   "source": [
    "train['SAOldPeopleSesothoVSSetswana'] = np.absolute(train['lan_06'] - train['lan_07'])\n",
    "\n",
    "test['SAOldPeopleSesothoVSSetswana'] = np.absolute(test['lan_06'] - test['lan_07'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k47k7b0_6OGL"
   },
   "source": [
    "###### Target encoding + PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecbR6lup6CgU"
   },
   "outputs": [],
   "source": [
    "target_mean = train.groupby(['cluster']).mean()[['target_pct_vunerable']]\n",
    "for i in list(target_mean.columns):\n",
    "  target_mean.rename({i:i+\"_mean\"},axis=1,inplace=True)\n",
    "train = train.merge(target_mean,how=\"left\",on='cluster')\n",
    "test = test.merge(target_mean,how=\"left\",on='cluster')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1eLDTE4h5rId"
   },
   "outputs": [],
   "source": [
    "pca = PCA(random_state=42,n_components=1)\n",
    "pg_features =  train.filter(regex='lan_.*')\n",
    "train_pca = pca.fit_transform(pg_features)\n",
    "train['pca_lan_0'] = train_pca[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-qO-JbF6HEV"
   },
   "outputs": [],
   "source": [
    "pg_features =  test.filter(regex='lan_.*')\n",
    "test_pca = pca.transform(pg_features)\n",
    "test['pca_lan_0'] = test_pca[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E2Aqs6VT6OGM"
   },
   "source": [
    "###### Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jptOCWh0Wocv"
   },
   "outputs": [],
   "source": [
    "target = train['target_pct_vunerable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_aJZuhzm_wx"
   },
   "outputs": [],
   "source": [
    "train = train.drop(['psa_00','psa_02','psa_03','psa_04','psa_01','lgt_00','stv_01','car_01','lln_01','target_pct_vunerable'], axis=1)\n",
    "test = test.drop(['psa_00','psa_02','psa_03','psa_04','psa_01','lgt_00','stv_01','car_01','lln_01'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGLALNrMC0xu"
   },
   "outputs": [],
   "source": [
    "X = train\n",
    "y = target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9NZb7zMDNxv"
   },
   "source": [
    "# RandomizedSearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLIJtQNNDKlJ"
   },
   "outputs": [],
   "source": [
    "models = [LinearRegression(), KNeighborsRegressor(),\n",
    "          LinearSVR(),SVR(),DecisionTreeRegressor(),\n",
    "          RandomForestRegressor(), GradientBoostingRegressor(),\n",
    "          XGBRegressor(), LGBMRegressor(),MLPRegressor(),\n",
    "          CatBoostRegressor(verbose = False), Ridge(), Lasso()]\n",
    "names = [\"LinearRegression\",\"KNN\",\"LinearSVR\",\"SVR\",\n",
    "             \"DecisionTree\",\"Random_Forest\",\"GBM\",\"XGBoost\",\"LightGBM\",\"Art.Neural_Network\",\"CatBoost\", \"Lasso\", 'Ridge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M7IK74yhDVXc"
   },
   "outputs": [],
   "source": [
    "# Possible hyper parameters\n",
    "linreg_params= {}\n",
    "\n",
    "knn_params= {\"model__n_neighbors\": np.linspace(1,19,10, dtype = int).tolist(),\n",
    "                 \"model__weights\": [\"uniform\",\"distance\"],\n",
    "                 \"model__metric\":[\"euclidean\",\"manhattan\"]}\n",
    "linearsvr_params={\"model__C\": np.arange(1,51),\n",
    "                  \"model__tol\": [0.0001, 0.0005, 0.00007, 0.00009, 0.005, 0.1, 0.001]}\n",
    "\n",
    "svr_params= {\"model__kernel\" : [\"rbf\"],\n",
    "                 \"model__gamma\": [0.001, 0.01, 0.1, 1, 5, 10 ,50 ,100],\n",
    "                 \"model__C\": [1,10,50,100,200,300,1000]}\n",
    "dtree_params = {\"model__min_samples_split\" : range(10,500,20),\n",
    "                \"model__max_depth\": range(1,20,2),\n",
    "                }\n",
    "rf_params = {\"model__max_features\": [\"log2\",\"Auto\",\"None\"],\n",
    "                \"model__min_samples_split\":[2,3,5],\n",
    "                \"model__min_samples_leaf\":[1,3,5],\n",
    "                \"model__bootstrap\":[True,False],\n",
    "                \"model__n_estimators\":[50,100,150]}\n",
    "gbm_params = {\"model__learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n",
    "             \"model__n_estimators\": [100,500,100],\n",
    "             \"model__max_depth\": [3,5,10],\n",
    "             \"model__min_samples_split\": [2,5,10]}\n",
    "gbm_params = {\"model__learning_rate\" : [0.001, 0.01, 0.1, 0.05],\n",
    "             \"model__n_estimators\": [100,500,100],\n",
    "             \"model__max_depth\": [3,5,10],\n",
    "             \"model__min_samples_split\": [2,5,10]}\n",
    "\n",
    "xgb_params ={\n",
    "                \"model__n_estimators\":[100,200,400],\n",
    "                 \"model__max_depth\":[1,2,3,4],\n",
    "                 'model__n_estimators': [50, 100, 200],\n",
    "                 'model__subsample': [ 0.6, 0.8, 1.0],\n",
    "                 'model__learning_rate': [0.1,0.2, 0.3, 0.4, 0.5],\n",
    "                 \"model__min_samples_split\": [1,2,4,6],\n",
    "                 \"model__random_state\": [42],\n",
    "                #  \"model_colsample_bytree\":[1.0,0.9,0.8]\n",
    "             }\n",
    "lgbm_params = { 'model__max_depth': [-1],\n",
    "        'model__n_estimators': [100, 500, 1000, 2000],\n",
    "        'model__subsample': [0.6, 0.8, 1.0],\n",
    "        'model__num_leaves':[31,45,60],\n",
    "        'model__learning_rate': [0.1,0.05,0.02,0.01],\n",
    "        \"model__min_child_samples\": [10,20,30]}\n",
    "mlpc_params = {\"model__alpha\": [0.1, 0.01, 0.02, 0.005, 0.0001,0.00001],\n",
    "              \"model__hidden_layer_sizes\": [(10,10,10),\n",
    "                                     (100,100,100),\n",
    "                                     (100,100),\n",
    "                                     (3,5), \n",
    "                                     (5, 3)],\n",
    "              \"model__solver\" : [\"lbfgs\",\"adam\",\"sgd\"],\n",
    "              \"model__activation\": [\"relu\",\"tanh\"]}\n",
    "catb_params =  {'model__depth':[2, 3, 4],\n",
    "              'model__loss_function': ['RMSE'],\n",
    "              'model__l2_leaf_reg':np.arange(2,31),\n",
    "              'model__depth':[3,1,2,6,4,5,7,8,9,10],\n",
    "              'model__iterations':[250,100,500,1000, 100000, 10000],\n",
    "              'model__learning_rate':[0.03,0.001,0.01,0.06, 0.05, 0.04, 0.03, 0.1,0.2,0.3],\n",
    "              'model__border_count':[32,5,10,20,50,100,200]}\n",
    "lasso_params = {'model__alpha':[0.02, 0.024, 0.025, 0.026, 0.03, 1, 10, 100, 1000]}\n",
    "ridge_params = {'model__alpha':[0.02, 0.024, 0.025, 0.026, 0.03, 1, 10, 100, 200, 230, 250,265, 270, 275, 290, 300, 500],\n",
    "                'model__max_iter':  [1, 10, 100, 1000, 500]}\n",
    "cluster_params = {\"clustering__n_clusters\":[37, 38, 39,40,41,42,43]}\n",
    "discretizer_params = {'discretizer__n_bins':[3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "                  'discretizer__strategy' : ['uniform', 'quantile', 'kmeans']}\n",
    "regressor_params = [linreg_params,knn_params, linearsvr_params,svr_params,dtree_params,rf_params,\n",
    "                     gbm_params, xgb_params,lgbm_params,mlpc_params,catb_params, lasso_params, ridge_params]               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IDcaEslsDbD-"
   },
   "outputs": [],
   "source": [
    "# Tuninig by Cross Validation  \n",
    "cv_result = {}\n",
    "best_estimators = {}\n",
    "for name, model,regressor_param in zip(names, models,regressor_params):\n",
    "    print(name)\n",
    "    pipe = Pipeline([\n",
    "                     ('transformer', QuantileTransformer()),                 \n",
    "                     ('discretizer', KBinsDiscretizer(encode='ordinal', n_bins=8)),\n",
    "                     ('scaler', RobustScaler()),\n",
    "                     ('model', model)])\n",
    "    regressor_param = {**regressor_param, **discretizer_params}\n",
    "    try:\n",
    "      reg = RandomizedSearchCV(pipe, regressor_param, cv =5, n_jobs =-1,verbose = 2, n_iter=10, scoring=mean_squared_error)\n",
    "      reg.fit(X,y)\n",
    "    except ValueError:\n",
    "      reg = RandomizedSearchCV(pipe, regressor_param, cv =5, n_jobs =-1,verbose = 2, n_iter=10)\n",
    "      reg.fit(X,y)\n",
    "    cv_result[name]=reg.best_score_\n",
    "    best_estimators[name]=reg.best_estimator_\n",
    "    print(cv_result)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0bIb0WVmaCni"
   },
   "source": [
    "# Voting Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhoaQ2vSDr_S"
   },
   "outputs": [],
   "source": [
    "#Keeping only the estimators with 0.75 as score in cross validation\n",
    "first_estimators = [(k,v) for k,v in best_estimators.items() if cv_result[k]>=0.75]\n",
    "\n",
    "# Checking the number of estimators retained\n",
    "print(len(first_estimators))\n",
    "\n",
    "# Fitting the data by vote with the estimators retained\n",
    "votingR = VotingRegressor(estimators = first_estimators, n_jobs =-1)\n",
    "\n",
    "votingR = votingR.fit(X, y)\n",
    "\n",
    "# Saving in pickle file the models with its parameters\n",
    "with open(path+f'voting_beta.csv.pkl', 'wb') as f:\n",
    "  pickle.dump(votingR, f)\n",
    "\n",
    "# making predictions on test data\n",
    "sub = test\n",
    "\n",
    "y_pred_grid = votingR.predict(sub)\n",
    "\n",
    "# making submissions\n",
    "submission_df = pd.DataFrame({'ward': test_ward, 'target_pct_vunerable ': y_pred_grid}) # Creating a submission file\n",
    "\n",
    "submission_df.to_csv(path+ 'Submissions/'+'gamma_voting.csv', index = False) #alpha 10 #beta 50 gamma 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfOKFuA3DNiX"
   },
   "outputs": [],
   "source": [
    "submission_df[\"target_pct_vunerable \"] = np.clip(submission_df[\"target_pct_vunerable \"], 0.099, 47)\n",
    "submission_df.to_csv(path+ 'Submissions/'+'gamma_voting_clipped.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9LDMV4jG1RA"
   },
   "outputs": [],
   "source": [
    "votingR.score(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPhr0O-3ZFsu"
   },
   "outputs": [],
   "source": [
    "submission_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EfT-mZFwZPDu"
   },
   "outputs": [],
   "source": [
    "s1 = pd.read_csv(path+\"Submissions/alpha_voting.csv\") #n_iter=10\n",
    "s2 = pd.read_csv(path+\"Submissions/beta_voting.csv\")  #n_iter=50\n",
    "submission_df['target_pct_vunerable ']=(s1['target_pct_vunerable ']+s2['target_pct_vunerable '])/2\n",
    "submission_df.to_csv(path+ 'Submissions/'+'alpha_beta.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o95SssIMpz95"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPTiotCO5NI0PTQN+qTMoaL",
   "collapsed_sections": [],
   "mount_file_id": "1rT81aouV0_XzAy4mHsqil9SzggTRcNFQ",
   "name": "Second Approach: Voting Regressor.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
